{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff2cee4-b491-4d25-a1c4-801b4d1b88ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule_with_warmup\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8b8cd05-e9cf-4606-b212-adfbfd7c43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "raw_data = []\n",
    "raw_data = pd.read_json(root + '/data/News_Category_Dataset_v2.json', lines=True)\n",
    "\n",
    "# data pre-processing\n",
    "def category_merge(x):\n",
    "    if x == 'THE WORLDPOST' or x == 'WORLD NEWS' or x == 'WORLDPOST':\n",
    "        return 'WORLD NEWS'\n",
    "    elif x == 'TASTE':\n",
    "        return 'FOOD & DRINK'\n",
    "    elif x == 'STYLE':\n",
    "        return 'STYLE & BEAUTY'\n",
    "    elif x == 'PARENTING':\n",
    "        return 'PARENTS'\n",
    "    elif x == 'COLLEGE':\n",
    "        return 'EDUCATION'\n",
    "    elif x == 'ARTS' or x == 'CULTURE & ARTS':\n",
    "        return 'ARTS & CULTURE'\n",
    "    elif x == 'SCIENCE' or x == 'TECH':\n",
    "        return 'SCIENCE & TECH'\n",
    "    elif x == 'BLACK VOICES' or x == 'DIVORCE' or x == 'LATINO VOICES' or x == 'QUEER VOICES' or x == 'GOOD NEWS':\n",
    "        return 'DROP'\n",
    "    elif x == 'WEDDINGS' or x == 'WOMEN' or x == 'IMPACT' or x == 'CRIME' or x == 'MEDIA' or x == 'WEIRD NEWS':\n",
    "        return 'DROP'\n",
    "    elif x == 'GREEN' or x == 'RELIGION' or x == 'EDUCATION' or x == 'MONEY' or x == 'FIFTY' or x == 'ENVIRONMENT':\n",
    "        return 'DROP'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def drop_small_data(data):\n",
    "    data['category'] = data['category'].apply(category_merge)\n",
    "    data[\"text\"] = data[\"headline\"] + '. ' + data[\"short_description\"]\n",
    "    data.drop([\"authors\", \"link\", \"date\", \"headline\", \"short_description\"], axis=1, inplace=True)\n",
    "\n",
    "    drop_data = []\n",
    "    cnt = 0\n",
    "    for i in data['category']:\n",
    "        if i == 'EDUCATION' or i == 'DROP':\n",
    "            drop_data.append(cnt)\n",
    "        cnt = cnt + 1\n",
    "    data.drop(index=drop_data, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def drop_large_data(data):\n",
    "    drop_large_data = []\n",
    "    cnt = [0, 0, 0, 0, 0]\n",
    "    index = 0\n",
    "\n",
    "    for i in data['category']:\n",
    "        if i == 'POLITICS':\n",
    "            cnt[0] = cnt[0] + 1\n",
    "            if cnt[0] > 10000:\n",
    "                drop_large_data.append(index)\n",
    "        elif i == 'WELLNESS':\n",
    "            cnt[1] = cnt[1] + 1\n",
    "            if cnt[1] > 10000:\n",
    "                drop_large_data.append(index)\n",
    "        elif i == 'PARENTS':\n",
    "            cnt[2] = cnt[2] + 1\n",
    "            if cnt[2] > 10000:\n",
    "                drop_large_data.append(index)\n",
    "        elif i == 'ENTERTAINMENT':\n",
    "            cnt[3] = cnt[3] + 1\n",
    "            if cnt[3] > 10000:\n",
    "                drop_large_data.append(index)\n",
    "        elif i == 'STYLE & BEAUTY':\n",
    "            cnt[4] = cnt[4] + 1\n",
    "            if cnt[4] > 10000:\n",
    "                drop_large_data.append(index)\n",
    "        index = index + 1\n",
    "\n",
    "    data.drop(index=drop_large_data, inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def pre_processing(data):\n",
    "    data = drop_small_data(data)\n",
    "    data = drop_large_data(data)\n",
    "    return data\n",
    "\n",
    "data = pre_processing(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04ebe71-fe9f-4762-b15c-94060f7e42d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# bert, tokenizer, device setting\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12126987-9371-4979-82b8-19bea0002dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_category = list(data[\"category\"].unique())\n",
    "\n",
    "def sparse_categories(category):\n",
    "    return int_category.index(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7acad7a-2ede-458c-91b8-e5ef376b3c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80844</th>\n",
       "      <td>Can Science Show Us Secrets Of Making Better D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52810</th>\n",
       "      <td>Global Travel Needs to Be Part of the Solution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>'The Late Show' Unveils Spoof Line Of Trump-Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37220</th>\n",
       "      <td>This Champagne Machine Gun Offers The Most Rid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104541</th>\n",
       "      <td>Menopause as a Consequence of Within-Family Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text\n",
       "80844   Can Science Show Us Secrets Of Making Better D...\n",
       "52810   Global Travel Needs to Be Part of the Solution...\n",
       "554     'The Late Show' Unveils Spoof Line Of Trump-Th...\n",
       "37220   This Champagne Machine Gun Offers The Most Rid...\n",
       "104541  Menopause as a Consequence of Within-Family Co..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = shuffle(data)\n",
    "\n",
    "#splitting data to train, val, test(7:2:1)\n",
    "train_text, val_text, train_category, val_category = train_test_split(data.drop(\"category\", axis=1), data[\"category\"], test_size=0.2)\n",
    "train_text, test_text, train_category, test_category = train_test_split(train_text, train_category, test_size=0.15)\n",
    "\n",
    "train_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f9e758-d470-4a13-b9f2-329221f9c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, ids, texts, targets, tokenizer, max_len):\n",
    "        self.ids = ids\n",
    "        self.texts = texts\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.ids[idx]\n",
    "        text = self.texts[idx]\n",
    "        label = self.targets[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True,\n",
    "                                              max_length=self.max_len,\n",
    "                                              return_token_type_ids=False,\n",
    "                                              padding='max_length',\n",
    "                                              return_attention_mask=True,\n",
    "                                              truncation=True,\n",
    "                                              return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "          'id': torch.tensor(id, dtype=torch.long),\n",
    "          'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'],\n",
    "          'label': torch.tensor(label, dtype=torch.int)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b08c3d-59e9-4072-a89e-c147db6d8040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load\n",
    "\n",
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_data = TextDataset(texts=train_text.text.to_numpy(), targets=train_category.to_numpy(),\n",
    "                         ids=train_text.index.to_numpy(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "val_data = TextDataset(texts=val_text.text.to_numpy(), targets=val_category.to_numpy(),\n",
    "                       ids=val_text.index.to_numpy(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "test_data = TextDataset(texts=test_text.text.to_numpy(), targets=test_category.to_numpy(),\n",
    "                        ids=test_text.index.to_numpy(), tokenizer=tokenizer, max_len=MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, num_workers=1, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, num_workers=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, num_workers=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a93f1f-7f12-490d-abe8-7205d1baff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classfier model\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, BERT, _dropout=0.3):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BERT\n",
    "        self.drop = nn.Dropout(_dropout)\n",
    "        self.classifier = nn.Linear(BERT.config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        pooled_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.drop(pooled_output[0][:, 0, :])\n",
    "        return self.classifier(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89116f32-b29d-45e2-ba4f-2e143df918a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation function\n",
    "def evaluation(model, data_loader, loss_function, device):\n",
    "    model = model.eval()\n",
    "    all_predictions , true_labels= [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            labels = data[\"label\"].to(device)\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            \n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "            loss = loss_function(output, labels.long())\n",
    "            all_predictions.append(preds.cpu().data)\n",
    "            true_labels.append(labels.cpu().data)\n",
    "    \n",
    "    all_predictions = np.concatenate(all_predictions, axis = 0)\n",
    "    true_labels = np.concatenate(true_labels, axis = 0)\n",
    "            \n",
    "    accuracy = accuracy_score(true_labels, all_predictions)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "452cae91-4d33-4eca-9345-5d8da39bea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(model, epochs, train_dataloader, val_dataloader,\n",
    "          loss_function, optimizer, device, scheduler):\n",
    "    \n",
    "    model = model.train()\n",
    "    accumulation_step = 4\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        all_predictions, true_labels = [], []\n",
    "        cnt = 0\n",
    "        \n",
    "        for data in tqdm(train_dataloader):\n",
    "            labels = data[\"label\"].to(device)\n",
    "            input_ids = data[\"input_ids\"].to(device)\n",
    "            attention_mask = data[\"attention_mask\"].to(device)\n",
    "            \n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(output, dim=1)\n",
    "            loss = loss_function(output, labels.long())\n",
    "            loss = loss / accumulation_step\n",
    "            all_predictions.append(preds.cpu().data)\n",
    "            true_labels.append(labels.cpu().data)\n",
    "            \n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            if (cnt + 1) % accumulation_step == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            cnt += 1\n",
    "            \n",
    "        all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "        true_labels = np.concatenate(true_labels, axis=0)\n",
    "        accuracy = accuracy_score(true_labels, all_predictions)\n",
    "        val_accuracy = evaluation(model, val_dataloader, loss_function, device)\n",
    "        \n",
    "        print(f\"Epoch: {epoch + 1} Accuracy: {accuracy}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea33540-ec07-4da1-b14d-3da4086de29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(len(int_category), bert, 0.4)\n",
    "model = model.to(device)\n",
    "EPOCHS = 1\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "warmup_step = int(len(train_dataloader) / 2) \n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=warmup_step,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27f011f7-2eda-4d9f-bf99-f88c0febcb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b357a3d92973482db7b28b0557f350e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8e7530672eed>:25: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(model.parameters(), max_norm = 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Accuracy: 0.40110903295487477\n",
      "Validation Accuracy: 0.7036854596748914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.40110903295487477"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model=model, epochs=EPOCHS, train_dataloader=train_dataloader, val_dataloader=val_dataloader,\n",
    "     loss_function=loss_function, optimizer=optimizer, device=device, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48faa31c-1cc8-4c64-866e-4e266562a1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071423240540339\n"
     ]
    }
   ],
   "source": [
    "print(evaluation(model, test_dataloader, loss_function, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dda06102-08df-418c-811d-42b6c2b3bcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./weights/classification.pth\"\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ac5f9-f94b-4ad0-b245-f22870d267bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
